{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dbf40b5",
   "metadata": {},
   "source": [
    "# Defining Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18332526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path.append('../examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8dc94",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aad954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch \n",
    "import torch\n",
    "import torchvision\n",
    "# General Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "# System and Path Libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Libraries for Tranforming into Yolov7 format data\n",
    "from yolov7.dataset import Yolov7Dataset\n",
    "from yolov7.dataset import create_yolov7_transforms\n",
    "from yolov7.plotting import show_image\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries for Training:\n",
    "from functools import partial\n",
    "from func_to_script import script\n",
    "from PIL import Image\n",
    "from pytorch_accelerated.callbacks import (\n",
    "    ModelEmaCallback,\n",
    "    ProgressBarCallback,\n",
    "    SaveBestModelCallback,\n",
    "    get_default_callbacks,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from pytorch_accelerated.schedulers import CosineLrScheduler\n",
    "from torch.utils.data import Dataset\n",
    "from yolov7 import create_yolov7_model\n",
    "from yolov7.dataset import (\n",
    "    Yolov7Dataset,\n",
    "    create_base_transforms,\n",
    "    create_yolov7_transforms,\n",
    "    yolov7_collate_fn,\n",
    ")\n",
    "from yolov7.evaluation import CalculateMeanAveragePrecisionCallback\n",
    "from yolov7.loss_factory import create_yolov7_loss\n",
    "from yolov7.mosaic import MosaicMixupDataset, create_post_mosaic_transform\n",
    "from yolov7.trainer import Yolov7Trainer, filter_eval_predictions\n",
    "from yolov7.utils import SaveBatchesCallback, Yolov7ModelEma\n",
    "\n",
    "######################\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Printing the versions\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43c945-d45c-4b58-895f-9fd76775dc36",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebd293-e06f-4fa5-a0a0-0fe93a0d0b80",
   "metadata": {},
   "source": [
    "First, let's take a look at how to load our dataset in the format that Yolov7 expects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cae8d4",
   "metadata": {},
   "source": [
    "## Selecting a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbfefe-6c3c-4f13-b6a8-9b7230d94669",
   "metadata": {},
   "source": [
    "Throughout this article, we shall use the [Kaggle cars object detection dataset](https://www.kaggle.com/datasets/sshikamaru/car-object-detection); however, as our aim is to demonstrate how Yolov7 can be applied to any problem, this is really the least important part of this work. Additionally, as the images are quite similar to COCO, it will enable us to experiment with a pretrained model before we do any training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984e66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa32f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the path for the video on which inference is needed\n",
    "data_path = \"C://Users//endo//Desktop//Yolov7-training-main//Yolov7-training-main//data//papilla\"\n",
    "video_path = data_path +\"//\"+ \"cropped_ben_ami_exalt_1.mp4\" # all_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dcf170",
   "metadata": {},
   "source": [
    "# Converting the Video into required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b7d5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import video\n",
    "# Split it into frames and save the frames (give numbering to the frame sequentially)\n",
    "# Build a dataframe, and sort it such that the rows are in increasing order of the frames\n",
    "# Convert the datat into Dataset Adaptor\n",
    "# Now transform the data into Yolov7 format\n",
    "# Return the final list of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "598f7267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_accelerated:Setting random seeds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from func_to_script import script\n",
    "from PIL import Image\n",
    "from pytorch_accelerated.callbacks import (\n",
    "    EarlyStoppingCallback,\n",
    "    SaveBestModelCallback,\n",
    "    get_default_callbacks,\n",
    ")\n",
    "from pytorch_accelerated.schedulers import CosineLrScheduler\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from yolov7 import create_yolov7_model\n",
    "from yolov7.dataset import Yolov7Dataset, create_yolov7_transforms, yolov7_collate_fn\n",
    "from yolov7.evaluation import CalculateMeanAveragePrecisionCallback\n",
    "from yolov7.loss_factory import create_yolov7_loss\n",
    "from yolov7.trainer import Yolov7Trainer, filter_eval_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afdafbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code: Where the inference is ran\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from train_cars import CarsDatasetAdaptor\n",
    "from yolov7.dataset import Yolov7Dataset\n",
    "from yolov7.dataset import create_yolov7_transforms\n",
    "\n",
    "def extract_frames_from_video(video_path, output_directory):\n",
    "    \n",
    "    ## Values for cropping: These values could be different if a different SUD is used (this is for Exalt D)\n",
    "    xmin = 560  \n",
    "    ymin = 60 \n",
    "    xmax = 1680  \n",
    "    ymax = 1027 \n",
    "    \n",
    "    my_frames_path= video_path.split(\".\")[-2].split(\"\\\\\")[-1] + \"_frames\"\n",
    "        # Check if the output directory exists\n",
    "    if not os.path.exists(my_frames_path):\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(my_frames_path)\n",
    "        # Open the video file\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Initialize variables\n",
    "        frame_count = 0\n",
    "        success = True\n",
    "        while success:\n",
    "            # Read the next frame from the video\n",
    "            success, frame = video.read()\n",
    "            if success:\n",
    "                ## Cropping the frame\n",
    "                cropped_frame = frame[ymin:ymax, xmin:xmax]\n",
    "                # Save the frame as an image file\n",
    "                frame_path = os.path.join(my_frames_path, f\"frame_{frame_count}.jpg\")\n",
    "                cv2.imwrite(frame_path, cropped_frame)\n",
    "    #             print(f\"Frame {frame_count} saved.\")\n",
    "                frame_count += 1\n",
    "        # Release the video file\n",
    "        video.release()\n",
    "    else:\n",
    "        print(f\"Output directory '{my_frames_path}' already exists.\")\n",
    "    \n",
    "    # Creating dataframe\n",
    "    my_list= os.listdir(my_frames_path)\n",
    "#     print(my_list)\n",
    "    i=0\n",
    "    ## Creating dataframe    \n",
    "    my_df= {'image':[], 'xmin':[],'ymin':[], 'xmax':[], 'ymax':[], 'class_name':[], 'has_annotation':[], 'image_id':[], 'class_id':[]}\n",
    "\n",
    "    for image_name in my_list:\n",
    "#         print(image_name)\n",
    "        i=i+1\n",
    "        my_df['image'].append(image_name)\n",
    "        my_df['class_name'].append('doesnt_matter')\n",
    "        my_df['class_id'].append(np.nan)\n",
    "        my_df['image_id'].append(i)\n",
    "        my_df['has_annotation'].append(False)\n",
    "        my_df['xmin'].append(np.nan)\n",
    "        my_df['ymin'].append(np.nan)\n",
    "        my_df['xmax'].append(np.nan)\n",
    "        my_df['ymax'].append(np.nan)\n",
    "        \n",
    "        \n",
    "#     print(my_df)\n",
    "    ### Getting the corresponding bbox values from annotations based on the id from data['images]:\n",
    "    # Building dataframe \n",
    "    my_final_df=  pd.DataFrame(my_df)\n",
    "    my_final_df['image_id']= my_final_df['image'].apply(lambda x: int(x.split('.')[0].split('_')[1]))\n",
    "    # Sorting the dataframe: this is becasue we want the video frame in sequential order\n",
    "    final_df = my_final_df.sort_values(by='image_id').reset_index().drop(columns= 'index')\n",
    "    display(final_df)\n",
    "    \n",
    "    # Converting dataframe to dataset adaptor\n",
    "    my_test_ds= CarsDatasetAdaptor(my_frames_path, final_df)\n",
    "    # Converting into yolov7 format (tranforming)\n",
    "    target_image_size= 640\n",
    "    my_test_yds= Yolov7Dataset(my_test_ds, transforms=create_yolov7_transforms(image_size=(target_image_size, target_image_size)))\n",
    "    return my_test_yds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60546142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class_name</th>\n",
       "      <th>has_annotation</th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>frame_0.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doesnt_matter</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frame_1.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doesnt_matter</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frame_2.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doesnt_matter</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frame_3.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doesnt_matter</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>frame_4.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doesnt_matter</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>frame_1522.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doesnt_matter</td>\n",
       "      <td>False</td>\n",
       "      <td>1522</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>frame_1523.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doesnt_matter</td>\n",
       "      <td>False</td>\n",
       "      <td>1523</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>frame_1524.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doesnt_matter</td>\n",
       "      <td>False</td>\n",
       "      <td>1524</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>frame_1525.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doesnt_matter</td>\n",
       "      <td>False</td>\n",
       "      <td>1525</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>frame_1526.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doesnt_matter</td>\n",
       "      <td>False</td>\n",
       "      <td>1526</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1527 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               image  xmin  ymin  xmax  ymax     class_name  has_annotation  \\\n",
       "0        frame_0.jpg   NaN   NaN   NaN   NaN  doesnt_matter           False   \n",
       "1        frame_1.jpg   NaN   NaN   NaN   NaN  doesnt_matter           False   \n",
       "2        frame_2.jpg   NaN   NaN   NaN   NaN  doesnt_matter           False   \n",
       "3        frame_3.jpg   NaN   NaN   NaN   NaN  doesnt_matter           False   \n",
       "4        frame_4.jpg   NaN   NaN   NaN   NaN  doesnt_matter           False   \n",
       "...              ...   ...   ...   ...   ...            ...             ...   \n",
       "1522  frame_1522.jpg   NaN   NaN   NaN   NaN  doesnt_matter           False   \n",
       "1523  frame_1523.jpg   NaN   NaN   NaN   NaN  doesnt_matter           False   \n",
       "1524  frame_1524.jpg   NaN   NaN   NaN   NaN  doesnt_matter           False   \n",
       "1525  frame_1525.jpg   NaN   NaN   NaN   NaN  doesnt_matter           False   \n",
       "1526  frame_1526.jpg   NaN   NaN   NaN   NaN  doesnt_matter           False   \n",
       "\n",
       "      image_id  class_id  \n",
       "0            0       NaN  \n",
       "1            1       NaN  \n",
       "2            2       NaN  \n",
       "3            3       NaN  \n",
       "4            4       NaN  \n",
       "...        ...       ...  \n",
       "1522      1522       NaN  \n",
       "1523      1523       NaN  \n",
       "1524      1524       NaN  \n",
       "1525      1525       NaN  \n",
       "1526      1526       NaN  \n",
       "\n",
       "[1527 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the path to your MP4 file\n",
    "my_video_path = video_path\n",
    "# Specify the output directory where frames will be saved\n",
    "output_directory = data_path\n",
    "# Call the function to extract frames\n",
    "yolo_format_data= extract_frames_from_video(video_path, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e4c1e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred 555/566 items from https://github.com/Chris-hughes10/Yolov7-training/releases/download/0.1.0/yolov7_training_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "# Defining model \n",
    "best_model = create_yolov7_model('yolov7', num_classes=1)\n",
    "best_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068d6a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "129b2eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading Weights\n",
    "# Change the pt file (weights file) here\n",
    "best_model_path= 'C:\\\\Users\\\\endo\\\\Desktop\\\\Yolov7-training-main\\\\Yolov7-training-main\\\\examples\\\\v7_annotations_finetune.pt'\n",
    "checkpoint = torch.load(best_model_path)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "best_model.load_state_dict(state_dict) # Loading the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe9caa",
   "metadata": {},
   "source": [
    "## Running inference on Video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c1bcef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of image tensors: DOing this to store all the image which are in the form of - cont\n",
    "#tensors into a list sequrntially and the passing each tensor to the model to obtain ouptut (inferencd)\n",
    "image_tensor_collection= []\n",
    "for i in range(len(yolo_format_data)):\n",
    "    image_tensor, labels, image_id, image_size = yolo_format_data[i]\n",
    "    image_tensor_collection.append(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a25347df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\endo\\Desktop\\Yolov7-training-main\\Yolov7-training-main\\examples\n"
     ]
    }
   ],
   "source": [
    "%cd \"C:\\Users\\endo\\Desktop\\Yolov7-training-main\\Yolov7-training-main\\examples\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54e58c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\endo\\anaconda3\\envs\\yolov7_custon\\lib\\site-packages\\torch\\functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Set up video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "## CHange the video output name here \n",
    "output_inference_file_name= \"output__cropped_ben_ami_exalt_1_finetune.mp4\"\n",
    "out = cv2.VideoWriter(output_inference_file_name, fourcc, 30.0, (640, 640)) # Set the name of the output file \n",
    "\n",
    "# Loop over the image tensors\n",
    "for image_tensor in image_tensor_collection:\n",
    "    # Perform inference on the image tensor\n",
    "    with torch.no_grad():\n",
    "        model_outputs = best_model(image_tensor[None])\n",
    "        # Postprocess the output to get the predictions\n",
    "        preds = best_model.postprocess(model_outputs, conf_thres=0., multiple_labels_per_box=False)\n",
    "\n",
    "    # Filter the predictions using NMS and a confidence threshold\n",
    "    nms_predictions = filter_eval_predictions(preds, confidence_threshold=0.1)\n",
    "    \n",
    "    # In some cases we can have multiple bounding boxes even after filtering which are above the set confidance threshold\n",
    "    # So in that case choosing only one bounding box from them\n",
    "    if len(nms_predictions[0])>1:\n",
    "        nms_predictions= [nms_predictions[0][1].reshape(1,6)]\n",
    "        \n",
    "    # Get the predicted boxes\n",
    "    pred_boxes = nms_predictions[0][:, :4].cpu().numpy()\n",
    "\n",
    "    # Load the image as a NumPy array\n",
    "    img = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Scale pixel values to range of 0 to 255\n",
    "    img = (img * 255).astype(np.uint8)# This was an important step, because all pixel values \n",
    "    #in the image_tensor were normalized so we need to scale it up first and then convert the format into uint8 \n",
    "    #(uint8 format is important for the writing into video file)\n",
    "    \n",
    "    # Resizing just to make sure\n",
    "    img= cv2.resize(img, (640, 640))\n",
    "\n",
    "    # Convert color space to BGR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw the predicted boxes on the image\n",
    "    for box in pred_boxes:\n",
    "        xmin, ymin, xmax, ymax = box.astype(int)\n",
    "        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "\n",
    "    ## Visualization\n",
    "#     window_name = 'image'\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     # Show the image with the predicted boxes\n",
    "#     cv2.imshow(window_name, img)\n",
    "#     cv2.waitKey(0)\n",
    "\n",
    "    # Write frame to video\n",
    "    out.write(img)\n",
    "\n",
    "\n",
    "# Release the video writer and close all windows\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e9061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov7_custon",
   "language": "python",
   "name": "yolov7_custon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
