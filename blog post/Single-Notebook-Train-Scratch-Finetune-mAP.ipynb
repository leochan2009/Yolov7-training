{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc80d7b3",
   "metadata": {},
   "source": [
    "# Defining Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8ec2d-144a-4b7c-96eb-e8f5c9021315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path.append('../examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8aa12e",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch \n",
    "import torch\n",
    "import torchvision\n",
    "# General Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "# System and Path Libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Libraries for Tranforming into Yolov7 format data\n",
    "from train_cars import CarsDatasetAdaptor\n",
    "from yolov7.dataset import Yolov7Dataset\n",
    "from yolov7.dataset import create_yolov7_transforms\n",
    "from yolov7.plotting import show_image\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries for Training:\n",
    "from functools import partial\n",
    "from func_to_script import script\n",
    "from PIL import Image\n",
    "from pytorch_accelerated.callbacks import (\n",
    "    ModelEmaCallback,\n",
    "    ProgressBarCallback,\n",
    "    SaveBestModelCallback,\n",
    "    get_default_callbacks,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from pytorch_accelerated.schedulers import CosineLrScheduler\n",
    "from torch.utils.data import Dataset\n",
    "from yolov7 import create_yolov7_model\n",
    "from yolov7.dataset import (\n",
    "    Yolov7Dataset,\n",
    "    create_base_transforms,\n",
    "    create_yolov7_transforms,\n",
    "    yolov7_collate_fn,\n",
    ")\n",
    "from yolov7.evaluation import CalculateMeanAveragePrecisionCallback\n",
    "from yolov7.loss_factory import create_yolov7_loss\n",
    "from yolov7.mosaic import MosaicMixupDataset, create_post_mosaic_transform\n",
    "from yolov7.trainer import Yolov7Trainer, filter_eval_predictions\n",
    "from yolov7.utils import SaveBatchesCallback, Yolov7ModelEma\n",
    "\n",
    "# FLOPs calculation Libraries\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "######################\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Printing the versions\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43c945-d45c-4b58-895f-9fd76775dc36",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12baaf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the file names as needed\n",
    "main_folder= \"anatomical-model-papilla-yolo-export-v1_dheeraj\"\n",
    "images_folder= 'Aslanian_Exalt_1_images'\n",
    "annotations_file_name= \"output_coco_annotations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c76f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "data_path = \"../data/papilla/\" + main_folder\n",
    "data_path = Path(data_path)\n",
    "images_path = data_path / images_folder # all_images\n",
    "annotations_file_path = data_path / annotations_file_name # annotations_final.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9956abef",
   "metadata": {},
   "source": [
    "# Crop the Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42707186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "images_list= os.listdir(images_path)## Creatng list of images from the actual image directory\n",
    "print(f\"Number of Images:{len(images_list)}\")\n",
    "# Verify coordinate values--> Change these values according to the image\n",
    "####(tip- can use paint app to find the xmin,ymin,xmax,ymax)\n",
    "\n",
    "xmin = 560  # example xmin value\n",
    "ymin = 60  # example ymin value\n",
    "xmax = 1680  # example xmax value\n",
    "ymax = 1027  # example ymax value\n",
    "\n",
    "images_list = os.listdir(images_path)\n",
    "## Defining the directory for croped images\n",
    "cropped_images_dir  = data_path / str(\"cropped_images-of-\"+str(images_path).split(\"\\\\\")[-1]) \n",
    "\n",
    "if not os.path.exists(cropped_images_dir):\n",
    "    os.makedirs(cropped_images_dir)\n",
    "    print(f\"Created directory: {cropped_images_dir}\")\n",
    "    \n",
    "    for image_name in images_list:\n",
    "        # Load the image\n",
    "        image_path = images_path / image_name\n",
    "        \n",
    "        image = cv2.imread(str(image_path))\n",
    "\n",
    "        # Perform cropping\n",
    "        cropped_image = image[ymin:ymax, xmin:xmax]\n",
    "        # Save the cropped image\n",
    "        cropped_image_path = cropped_images_dir / image_name\n",
    "        cv2.imwrite(str(cropped_image_path), cropped_image)\n",
    "#         print(f\"Cropped image saved: {cropped_image_path}\")\n",
    "#         break  # Uncomment this line if you want to crop only the first image for testing purposes\n",
    "\n",
    "else:\n",
    "    print(f\"Directory already exists: {cropped_images_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010e9f8",
   "metadata": {},
   "source": [
    "## Reading the Json file, Updating Bounding Box, Creating Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c1b6b5",
   "metadata": {},
   "source": [
    "### This function is used to get dataframe while using Coco-Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a91b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have the filename: Extracting other information from data['annotations']\n",
    "#But file name is present in data['images']\n",
    "# We need to search for the ID in data['images'] wrt to the filename\n",
    "# Then map each id with its respective image_id in data['annotations']--> from here we can extarct the information \n",
    "#LIke bbox, category_id \n",
    "# Updating the Bounding boxes and Creating the Dataframe\n",
    "def get_dataframe_coco_annotator(annotations_file_path):\n",
    "    # Reading the Annotation File\n",
    "    with open(annotations_file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "    papilla_images_list = os.listdir(images_path)\n",
    "\n",
    "    ## Creating dataframe    \n",
    "    my_df= {'image':[], 'xmin':[],'ymin':[], 'xmax':[], 'ymax':[], 'class_name':[], 'has_annotation':[], 'image_id':[], 'class_id':[]}\n",
    "    for filename in papilla_images_list:\n",
    "        my_id= [d for d in data['images'] if d['file_name']== filename][0]['id']\n",
    "        my_data= [d for d in data['annotations'] if d['image_id']== my_id] \n",
    "        filtered_data = [d for d in my_data if d['category_id'] != 216][0]    \n",
    "    #     print(filtered_data)\n",
    "        my_df['image'].append(filename)\n",
    "        my_df['image_id'].append(np.nan)\n",
    "\n",
    "        if filtered_data['category_id']==168:\n",
    "\n",
    "            my_df['class_name'].append('background')\n",
    "            my_df['class_id'].append(np.nan)\n",
    "            my_df['has_annotation'].append(False)## Im assuming if its a background (category id 168) there is no annotation\n",
    "\n",
    "            my_df['xmin'].append(np.nan)\n",
    "            my_df['ymin'].append(np.nan)\n",
    "            my_df['xmax'].append(np.nan)\n",
    "            my_df['ymax'].append(np.nan)\n",
    "\n",
    "        else:\n",
    "            # Reading Bounding Box\n",
    "            original_bbox= filtered_data['bbox']# The bbox is in the format [xmin, ymin, w, h]\n",
    "    ##################################################################################################        \n",
    "            # Converting bbox according to the cropped image and converting the format to [xmin, ymin, xmax, ymax]\n",
    "            crop_width = xmax - xmin\n",
    "            crop_height = ymax - ymin\n",
    "\n",
    "            new_xmin = max(0, original_bbox[0] - xmin)\n",
    "            new_ymin = max(0, original_bbox[1] - ymin)\n",
    "            new_xmax = min(crop_width, original_bbox[0] + original_bbox[2] - xmin)\n",
    "            new_ymax = min(crop_height, original_bbox[1] + original_bbox[3] - ymin)\n",
    "    ################################################################################################\n",
    "\n",
    "            my_df['class_name'].append('papilla')\n",
    "            my_df['class_id'].append(0.0)\n",
    "            my_df['has_annotation'].append(True)\n",
    "\n",
    "            my_df['xmin'].append(new_xmin)\n",
    "            my_df['ymin'].append(new_ymin)\n",
    "            my_df['xmax'].append(new_xmax)# The bounding boxes are in the format x,y,w,g, from coco annotater\n",
    "            my_df['ymax'].append(new_ymax)# Hence converting w & h to xmax, ymax \n",
    "    \n",
    "    final_df=  pd.DataFrame.from_dict(my_df)\n",
    "    final_df.image_id= list(range(1, len(final_df)+1))\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59163862",
   "metadata": {},
   "source": [
    "### This function is used to get dataframe while using V7 Annotator- coco annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the Bounding boxes and Creating the Dataframe\n",
    "def get_dataframe_v7_annotator(annotations_file_path):\n",
    "    with open(annotations_file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    annotations= data['annotations']        \n",
    "    my_images_list = os.listdir(data_path / \"Aslanian_Exalt_1_images\")\n",
    "\n",
    "    #Creating dataframe    \n",
    "\n",
    "    my_df= {'image':[], 'xmin':[],'ymin':[], 'xmax':[], 'ymax':[], 'class_name':[], \n",
    "            'has_annotation':[], 'image_id':[], 'class_id':[] }\n",
    "    # Iterate over the images\n",
    "    for image_filename in my_images_list:\n",
    "        # Find the annotation for the current image filename\n",
    "        new_image_filename = int(image_filename.split('.')[0])\n",
    "        matching_data= [d for d in data['annotations'] if d['image_id']== new_image_filename]\n",
    "        \n",
    "        ## Inputting data into dictionary for the dataframe building\n",
    "        my_df['image'].append(image_filename)\n",
    "        my_df['image_id'].append(np.nan)\n",
    "\n",
    "        if len(matching_data)==0:\n",
    "            my_df['class_name'].append('background')\n",
    "            my_df['class_id'].append(np.nan)\n",
    "            my_df['has_annotation'].append(False)## Im assuming if its a background (category id 168) there is no annotation\n",
    "\n",
    "            my_df['xmin'].append(np.nan)\n",
    "            my_df['ymin'].append(np.nan)\n",
    "            my_df['xmax'].append(np.nan)\n",
    "            my_df['ymax'].append(np.nan)\n",
    "\n",
    "        else:\n",
    "            # Reading Bounding Box\n",
    "            original_bbox= matching_data[0]['bbox']# The bbox is in the format [xmin, ymin, w, h]\n",
    "    ##################################################################################################        \n",
    "            # Converting bbox according to the cropped image and converting the format to [xmin, ymin, xmax, ymax]\n",
    "            crop_width = xmax - xmin\n",
    "            crop_height = ymax - ymin\n",
    "\n",
    "            new_xmin = max(0, original_bbox[0] - xmin)\n",
    "            new_ymin = max(0, original_bbox[1] - ymin)\n",
    "            new_xmax = min(crop_width, original_bbox[0] + original_bbox[2] - xmin)\n",
    "            new_ymax = min(crop_height, original_bbox[1] + original_bbox[3] - ymin)\n",
    "    ################################################################################################\n",
    "\n",
    "            my_df['class_name'].append('papilla')\n",
    "            my_df['class_id'].append(0.0)\n",
    "            my_df['has_annotation'].append(True)\n",
    "\n",
    "            my_df['xmin'].append(new_xmin)\n",
    "            my_df['ymin'].append(new_ymin)\n",
    "            my_df['xmax'].append(new_xmax)# The bounding boxes are in the format x,y,w,g, from coco annotater\n",
    "            my_df['ymax'].append(new_ymax)# Hence converting w & h to xmax, ymax \n",
    "    \n",
    "    final_df=  pd.DataFrame.from_dict(my_df)\n",
    "    final_df.image_id= list(range(1, len(final_df)+1))\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505fdb1",
   "metadata": {},
   "source": [
    "### Call anyone of the above two functions based on the type of annotation file being used: COCO or V7 COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc84f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df= get_dataframe_v7_annotator(annotations_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations_df= get_dataframe_coco_annotator(annotations_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa349c0",
   "metadata": {},
   "source": [
    "## Now continuing using the cropped image path and the annotation (dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cars_df(df):\n",
    "        \n",
    "    image_id_to_image = {i: im for i, im in zip(df.image_id, df.image)}\n",
    "    image_to_image_id = {v: k for k, v, in image_id_to_image.items()}\n",
    "    \n",
    "    class_id_to_label = dict(\n",
    "        enumerate(df.query(\"has_annotation == True\").class_name.unique())\n",
    "    )\n",
    "    class_label_to_id = {v: k for k, v in class_id_to_label.items()}\n",
    "        \n",
    "    # Splitting Data into 70-15-15--> Train, Validation and Test\n",
    "    train_df, valid_test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    valid_df, test_df = train_test_split(valid_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "    lookups = {\n",
    "        \"image_id_to_image\": image_id_to_image,\n",
    "        \"image_to_image_id\": image_to_image_id,\n",
    "        \"class_id_to_label\": class_id_to_label,\n",
    "        \"class_label_to_id\": class_label_to_id,\n",
    "    }\n",
    "    return train_df, valid_df, test_df, lookups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f2f4e-983b-4b70-b289-2beb8059d21a",
   "metadata": {},
   "source": [
    "We can now use this function to load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df, test_df, lookups = load_cars_df(annotations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ee0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fdbb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bbd01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.image.nunique(), valid_df.image.nunique(), test_df.image.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163741fe-fa70-4b75-9e00-0acf9772250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833892c8-a792-461a-b73e-506a71f5c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups['class_label_to_id'], lookups['class_id_to_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3be48e-aa09-482a-b9e5-9ea99a62da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num. annotated images in training set: {len(train_df.query('has_annotation == True').image.unique())}\")\n",
    "print(f\"Num. Background images in training set: {len(train_df.query('has_annotation == False').image.unique())}\")\n",
    "print(f\"Total Num. images in training set: {len(train_df.image.unique())}\")\n",
    "print('------------')\n",
    "\n",
    "print(f\"Num. annotated images in validation set: {len(valid_df.query('has_annotation == True').image.unique())}\")\n",
    "print(f\"Num. Background images in validation set: {len(valid_df.query('has_annotation == False').image.unique())}\")\n",
    "print(f\"Total Num. images in validation set: {len(valid_df.image.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ffa5b8",
   "metadata": {},
   "source": [
    "# Create a Dataset Adaptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f825aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CarsDatasetAdaptor(cropped_images_dir, train_df)\n",
    "valid_ds= CarsDatasetAdaptor(cropped_images_dir, valid_df)\n",
    "test_ds= CarsDatasetAdaptor(cropped_images_dir, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c894d9c4",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e17b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_size = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e502980",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_yds = Yolov7Dataset(train_ds, transforms=create_yolov7_transforms(image_size=(target_image_size, target_image_size)))\n",
    "eval_yds= Yolov7Dataset(valid_ds, transforms=create_yolov7_transforms(image_size=(target_image_size, target_image_size)))\n",
    "test_yds= Yolov7Dataset(test_ds, transforms=create_yolov7_transforms(image_size=(target_image_size, target_image_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2c7ad1",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c12eff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 16\n",
    "    \n",
    "image_tensor, labels, image_id, image_size = train_yds[idx]\n",
    "\n",
    "print(f'Image: {image_tensor.shape}')\n",
    "print(f'Labels: {labels}')\n",
    "\n",
    "# denormalize boxes\n",
    "boxes = labels[:, 2:]\n",
    "boxes[:, [0, 2]] *= target_image_size #image_size[1]## Multiplying with targetimagesize becasue, padding was applied (using transforms above)\n",
    "boxes[:, [1, 3]] *= target_image_size #image_size[0]\n",
    "\n",
    "show_image(image_tensor.permute( 1, 2, 0), boxes.tolist(), [lookups['class_id_to_label'][int(c)] for c in labels[:, 1]], 'cxcywh')\n",
    "plt.show()\n",
    "print(f'Image id: {image_id}')\n",
    "print(f'Image size: {image_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd71b8",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de303f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHangint the path: this is the path wehre the trained weight (.pt) file is created\n",
    "%cd \"C:\\Users\\endo\\Desktop\\Yolov7-training-main\\Yolov7-training-main\\examples\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f47527",
   "metadata": {},
   "source": [
    "# Training code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba33e4e4",
   "metadata": {},
   "source": [
    "## Note: Note that each time the trainign is run: a file named: \"best_model.pt\" is created so make sure to rename is before running the training- otherwise it gets overwritten. Both for finetuning and training from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dcf071",
   "metadata": {},
   "source": [
    "## Training from Scratch Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a977a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training from Scratch\n",
    "DATA_PATH =data_path\n",
    "\n",
    "def train_scratch(\n",
    "    train_ds, valid_ds,\n",
    "    data_path: str = DATA_PATH,\n",
    "    image_size: int = 640,\n",
    "    pretrained: bool = False,\n",
    "    num_epochs: int = 50, #was 50\n",
    "    batch_size: int = 8,\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu',# Add this line to set the device to use\n",
    "):\n",
    "##############################################################################\n",
    "    # CHecking on which device the training is going to run\n",
    "    print(device)\n",
    "    #Code added by Mike: print name of GPU\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "##############################################################################\n",
    "# Defining Number of classes\n",
    "    num_classes= 1 \n",
    "##############################################################################\n",
    "    # Mosaic Augmentation\n",
    "    mds = MosaicMixupDataset(\n",
    "        train_ds,\n",
    "        apply_mixup_probability=0.15,\n",
    "        post_mosaic_transforms=create_post_mosaic_transform(\n",
    "            output_height=image_size, output_width=image_size\n",
    "        ),\n",
    "    )\n",
    "##############################################################################\n",
    "    if pretrained:\n",
    "        # disable mosaic if finetuning\n",
    "        mds.disable()\n",
    "        print('Mosaic Disabled')\n",
    "##############################################################################\n",
    "    # Creating the Yolo Transformed data \n",
    "    train_yds = Yolov7Dataset(\n",
    "        mds,\n",
    "        create_yolov7_transforms(training=True, image_size=(image_size, image_size)),\n",
    "    )\n",
    "    eval_yds = Yolov7Dataset( \n",
    "        valid_ds,\n",
    "        create_yolov7_transforms(training=False, image_size=(image_size, image_size)),\n",
    "    )\n",
    "##############################################################################\n",
    "    # create model, loss function and optimizer\n",
    "    model = create_yolov7_model(\n",
    "        architecture=\"yolov7\", num_classes=num_classes, pretrained=pretrained\n",
    "    ).to(device) # Moving the model to device\n",
    "##############################################################################\n",
    "    param_groups = model.get_parameter_groups()\n",
    "##############################################################################\n",
    "    loss_func = create_yolov7_loss(model, image_size=image_size)\n",
    "##############################################################################\n",
    "    optimizer = torch.optim.SGD(\n",
    "        param_groups[\"other_params\"], lr=0.01, momentum=0.937, nesterov=True\n",
    "    )\n",
    "##############################################################################\n",
    "    # create evaluation callback and trainer\n",
    "    calculate_map_callback = (\n",
    "        CalculateMeanAveragePrecisionCallback.create_from_targets_df(\n",
    "            targets_df=valid_df.query(\"has_annotation == True\")[\n",
    "                [\"image_id\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"class_id\"]\n",
    "            ],\n",
    "            image_ids=set(valid_df.image_id.unique()),\n",
    "            iou_threshold=0.2,\n",
    "        )\n",
    "    )\n",
    "##############################################################################\n",
    "    trainer = Yolov7Trainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_func=loss_func,\n",
    "        filter_eval_predictions_fn=partial(\n",
    "            filter_eval_predictions, confidence_threshold=0.01, nms_threshold=0.3\n",
    "        ),\n",
    "        callbacks=[\n",
    "            calculate_map_callback,\n",
    "            SaveBestModelCallback(watch_metric=\"map\", greater_is_better=True),\n",
    "            SaveBatchesCallback(\"./batches\", num_images_per_batch=3),\n",
    "            *get_default_callbacks(progress_bar=True),\n",
    "        ],\n",
    "    )\n",
    "##############################################################################\n",
    "    # calculate scaled weight decay and gradient accumulation steps\n",
    "    total_batch_size = (\n",
    "        batch_size * trainer._accelerator.num_processes\n",
    "    )  # batch size across all processes\n",
    "\n",
    "    nominal_batch_size = 64\n",
    "    num_accumulate_steps = max(round(nominal_batch_size / total_batch_size), 1)\n",
    "    base_weight_decay = 0.0005\n",
    "    scaled_weight_decay = (\n",
    "        base_weight_decay * total_batch_size * num_accumulate_steps / nominal_batch_size\n",
    "    )\n",
    "\n",
    "    optimizer.add_param_group(\n",
    "        {\"params\": param_groups[\"conv_weights\"], \"weight_decay\": scaled_weight_decay}\n",
    "    )\n",
    "##############################################################################\n",
    "    # run training\n",
    "    trainer.train(\n",
    "        num_epochs=num_epochs,\n",
    "        train_dataset=train_yds,\n",
    "        eval_dataset=eval_yds,\n",
    "        per_device_batch_size=batch_size,\n",
    "        create_scheduler_fn=CosineLrScheduler.create_scheduler_fn(\n",
    "            num_warmup_epochs=5,\n",
    "            num_cooldown_epochs=5,\n",
    "            k_decay=2,\n",
    "        ),\n",
    "        collate_fn=yolov7_collate_fn,\n",
    "        gradient_accumulation_steps=num_accumulate_steps,\n",
    "    )\n",
    "##############################################################################\n",
    "## NOTE##\n",
    "# The mAP obtained here for the Validation set is when IoU is set for 0.2\n",
    "# Difference between finetuning and training from scratch code:\n",
    "# 1. Mosaic Augmentation is not applied in Finetuning\n",
    "# 2. Optimizer: model.get_parameter_groups() in Training from scratch; model.parameters() in FInetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54acf1e3",
   "metadata": {},
   "source": [
    "## Finetuning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuning the model\n",
    "DATA_PATH =data_path\n",
    "\n",
    "def train_finetune(\n",
    "    train_ds, valid_ds,\n",
    "    data_path: str = DATA_PATH,\n",
    "    image_size: int = 640,\n",
    "    pretrained: bool = True,\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 8,\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu',# Add this line to set the device to use\n",
    "):\n",
    "##############################################################################\n",
    "    # CHecking on which device the training is going to run\n",
    "    print(device)\n",
    "    #Code added by Mike: print name of GPU\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "##############################################################################\n",
    "# Defining Number of classes\n",
    "    num_classes= 1\n",
    "##############################################################################\n",
    "    train_yds = Yolov7Dataset(\n",
    "        train_ds,\n",
    "        create_yolov7_transforms(training=True, image_size=(image_size, image_size)),\n",
    "    )\n",
    "    eval_yds = Yolov7Dataset(\n",
    "        valid_ds,\n",
    "        create_yolov7_transforms(training=False, image_size=(image_size, image_size)),\n",
    "    )\n",
    "##############################################################################\n",
    "    # Create model, loss function and optimizer\n",
    "    model = create_yolov7_model(\n",
    "        architecture=\"yolov7\", num_classes=num_classes, pretrained=pretrained\n",
    "    ).to(device) # Dheeraj added .to(torch.device(\"cpu\")) \n",
    "##############################################################################\n",
    "\n",
    "    loss_func = create_yolov7_loss(model, image_size=image_size)\n",
    "    \n",
    "##############################################################################\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), lr=0.01, momentum=0.9, nesterov=True\n",
    "    )\n",
    "##############################################################################\n",
    "    # create evaluation callback and trainer\n",
    "    calculate_map_callback = (\n",
    "        CalculateMeanAveragePrecisionCallback.create_from_targets_df(\n",
    "            targets_df=valid_df.query(\"has_annotation == True\")[\n",
    "                [\"image_id\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"class_id\"]\n",
    "            ],\n",
    "            image_ids=set(valid_df.image_id.unique()),\n",
    "            iou_threshold=0.2,\n",
    "        )\n",
    "    )\n",
    "##############################################################################\n",
    "    # Create trainer and train\n",
    "    trainer = Yolov7Trainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_func=loss_func,\n",
    "        filter_eval_predictions_fn=partial(\n",
    "            filter_eval_predictions, confidence_threshold=0.01, nms_threshold=0.3\n",
    "        ),\n",
    "        callbacks=[\n",
    "            calculate_map_callback,\n",
    "            SaveBestModelCallback(watch_metric=\"map\", greater_is_better=True),\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=3,\n",
    "                watch_metric=\"map\",\n",
    "                greater_is_better=True,\n",
    "                early_stopping_threshold=0.001,\n",
    "            ),\n",
    "            *get_default_callbacks(progress_bar=True),\n",
    "        ],\n",
    "    )\n",
    "#############################################################################\n",
    "    trainer.train(\n",
    "        num_epochs=num_epochs,\n",
    "        train_dataset=train_yds,\n",
    "        eval_dataset=eval_yds,\n",
    "        per_device_batch_size=batch_size,\n",
    "        create_scheduler_fn=CosineLrScheduler.create_scheduler_fn(\n",
    "            num_warmup_epochs=5,\n",
    "            num_cooldown_epochs=5,\n",
    "            k_decay=2,\n",
    "        ),\n",
    "        collate_fn=yolov7_collate_fn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93680394",
   "metadata": {},
   "source": [
    "## Based on the type of training call that function: train_scratch or train_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_finetune(train_ds, valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577954c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_scratch(train_ds, valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6e5bf7",
   "metadata": {},
   "source": [
    "# Difference between Finetune and Training from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d0b43f",
   "metadata": {},
   "source": [
    "The notable differences between the \"Training from Scratch\" and \"Fine-tuning\" scenarios in the provided script:\n",
    "\n",
    "Fine-tuning the model:\n",
    "\n",
    "Dataset: The script defines train_yds and eval_yds using the Yolov7Dataset and create_yolov7_transforms functions for both training and evaluation datasets.\n",
    "\n",
    "Optimizer: The optimizer is created using model.parameters() instead of model.get_parameter_groups().\n",
    "Early Stopping: The script includes the EarlyStoppingCallback in the list of trainer callbacks, which helps stop training early if the metric does not improve.\n",
    "\n",
    "No Gradient Accumulation Steps: Unlike the \"Training from Scratch\" script, there is no calculation or mention of gradient accumulation steps in the \"Fine-tuning\" script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d670ea3",
   "metadata": {},
   "source": [
    "# Calculating mAP, Precision and Recall on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8cc598",
   "metadata": {},
   "source": [
    "## ***Note***\n",
    "#### I have created my own code for calculation of mAP, Precison, Recall, and Precision-Recall Curve\n",
    "#### But this might not be necessary if we can use the functions provided in the evaluation folder correctly\n",
    "#### Path for evaluation folder: C:\\Users\\endo\\Desktop\\Yolov7-training-main\\Yolov7-training-main\\yolov7\\evaluation\n",
    "#### Some idea on how to use it can be learnt from the above cell (Training Code), where the callbacks for calcualtion mAP were used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b837f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model \n",
    "best_model = create_yolov7_model('yolov7', num_classes=1)\n",
    "best_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f27d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the weights file name as needed\n",
    "model_weights_name= \"anatomical_data_Finetune_best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1845234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Weights\n",
    "best_model_path= 'C:\\\\Users\\\\endo\\\\Desktop\\\\Yolov7-training-main\\\\Yolov7-training-main\\\\examples\\\\'+ model_weights_name\n",
    "checkpoint = torch.load(best_model_path)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "best_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfddf36",
   "metadata": {},
   "source": [
    "# FLOPS Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da048e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample input tensor\n",
    "input_tensor = torch.randn(1, 3, 640, 640)  # Assuming input shape of (batch_size, channels, height, width)\n",
    "# Calculating\n",
    "macs, params = profile(best_model, inputs=(input_tensor,))\n",
    "# Because 1 MACS equals roughly 2 FLOPs\n",
    "flops= macs/2\n",
    "# Changing format\n",
    "flops, params = clever_format([flops, params], \"%.3f\")\n",
    "print(f\"FLOPs: {flops}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f90bc",
   "metadata": {},
   "source": [
    "## Running inference on test_yds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(test_yds)):\n",
    "    image_tensor, labels, image_id, image_size = test_yds[idx]\n",
    "    with torch.no_grad():\n",
    "        model_outputs = best_model(image_tensor[None])\n",
    "        preds = best_model.postprocess(model_outputs, conf_thres=0., multiple_labels_per_box=False)\n",
    "\n",
    "        # Inference\n",
    "        nms_predictions = filter_eval_predictions(preds, confidence_threshold=0.1)\n",
    "        nms_predictions[0].shape\n",
    "        pred_boxes = nms_predictions[0][:, :4]\n",
    "        class_ids = nms_predictions[0][:, -1]\n",
    "\n",
    "        show_image(image_tensor.permute( 1, 2, 0), pred_boxes.tolist(), class_ids.tolist())\n",
    "        plt.show()\n",
    "#         print(f'Image id: {image_id}')\n",
    "#         print(f'Original Image size: {image_size}')\n",
    "#         print(f'Resized Image size: {image_tensor.shape[1:]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ebc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels, box_format= \"corners\"):\n",
    "    # (N,4): N--number of bboxes\n",
    "    # boxes_labels shape is (N,4)\n",
    "    \n",
    "    '''Calculates intersection over union\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct Labels of Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "        '''       \n",
    "    if box_format == \"corners\":\n",
    "        \n",
    "        #Converting cx,cy,w,h (center_x, center_y,w,h) into (xmin, ymin, xmax, ymax)\n",
    "        \n",
    "#         print(boxes_labels)\n",
    "#         print(boxes_preds)\n",
    "        \n",
    "        center_x= boxes_labels[..., 0:1]\n",
    "        center_y = boxes_labels[..., 1:2] \n",
    "        width = boxes_labels[..., 2:3] \n",
    "        height= boxes_labels[..., 3:4] \n",
    "        new_boxes_labels = torch.zeros_like(boxes_labels) # Initializing the tensor\n",
    "\n",
    "        new_boxes_labels[..., 0:1]= center_x - (width / 2)\n",
    "        new_boxes_labels[..., 1:2]= center_y - (height / 2)\n",
    "        new_boxes_labels[..., 2:3]= center_x + (width / 2)\n",
    "        new_boxes_labels[..., 3:4]= center_y + (height / 2)        \n",
    "        \n",
    "        \n",
    "        pred_xmin= boxes_preds[..., 0:1]\n",
    "        pred_ymin= boxes_preds[..., 1:2]\n",
    "        pred_xmax= boxes_preds[..., 2:3]\n",
    "        pred_ymax= boxes_preds[..., 3:4]\n",
    "\n",
    "        label_xmin= new_boxes_labels[..., 0:1]\n",
    "        label_ymin= new_boxes_labels[..., 1:2]\n",
    "        label_xmax= new_boxes_labels[..., 2:3]\n",
    "        label_ymax= new_boxes_labels[..., 3:4]\n",
    "\n",
    "    inter_area = max(0, min(pred_xmax, label_xmax) - max(pred_xmin, label_xmin)) * \\\n",
    "             max(0, min(pred_ymax, label_ymax) - max(pred_ymin, label_ymin))\n",
    "\n",
    "    pred_area = (pred_xmax - pred_xmin) * (pred_ymax - pred_ymin)\n",
    "    label_area = (label_xmax - label_xmin) * (label_ymax - label_ymin)\n",
    "    union_area = pred_area + label_area - inter_area\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    conf= boxes_preds[...,4:5]\n",
    "    return iou, conf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f8449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Image\n",
    "df_list=[]\n",
    "iou_thres= 0.5\n",
    "for i in range(len(test_df)):\n",
    "    \n",
    "    # Taking a test image\n",
    "    image_tensor, labels, image_id, image_size = test_yds[i]\n",
    "    boxes_labels = labels[:, 2:]\n",
    "    boxes_labels[:, [0, 2]] *= target_image_size\n",
    "    boxes_labels[:, [1, 3]] *= target_image_size\n",
    "    \n",
    "    # Predicting \n",
    "    with torch.no_grad():\n",
    "        model_outputs = best_model(image_tensor[None])\n",
    "        preds = best_model.postprocess(model_outputs, conf_thres=0., multiple_labels_per_box=False)  \n",
    "        \n",
    "    # This has the bounding boxes with confidence and class label\n",
    "    nms_predictions = filter_eval_predictions(preds, confidence_threshold=0.1)\n",
    "    data= {'image_id':[],'gt_flag':[],'pd_flag':[], 'confidence':[], 'iou':[], 'tp':[], 'fp':[], 'fn':[], 'tn':[]}\n",
    "    \n",
    "    \n",
    "    # Chec if the Ground Truth Bbox is available:\n",
    "    if boxes_labels.numel()==0:\n",
    "        # Now check if Predicted Bounding boxes are zero or any got predicted:\n",
    "        if nms_predictions[0].numel()==0:\n",
    "#             print(\" We dont care about this case\")\n",
    "            data['image_id']= image_id.tolist()\n",
    "            data['gt_flag'].append(1)\n",
    "            data['pd_flag'].append(1)\n",
    "            data['confidence'].append(np.nan)\n",
    "            data['iou'].append(np.nan)\n",
    "            data['tp'].append(np.nan)\n",
    "            data['fn'].append(np.nan)\n",
    "            data['fp'].append(np.nan)\n",
    "            data['tn'].append(1)\n",
    "        elif nms_predictions[0].numel()!=0:\n",
    "            data['image_id']= image_id.tolist()\n",
    "            data['gt_flag'].append(1)\n",
    "            data['pd_flag'].append(0)\n",
    "            data['confidence'].append(np.nan)\n",
    "            data['iou'].append(np.nan)\n",
    "            data['tp'].append(np.nan)\n",
    "            data['fn'].append(np.nan)\n",
    "            data['fp'].append(1)\n",
    "            data['tn'].append(np.nan)\n",
    "    # this is when the ground truth bounding box is available:\n",
    "    else:\n",
    "        # Now we chck if the prediction are done or not:\n",
    "        # Checking if there are no predictions:\n",
    "        if nms_predictions[0].numel()==0:\n",
    "            data['image_id']= image_id.tolist()\n",
    "            data['gt_flag'].append(0)\n",
    "            data['pd_flag'].append(1)\n",
    "            data['confidence'].append(np.nan)\n",
    "            data['iou'].append(np.nan)\n",
    "            data['tp'].append(np.nan)\n",
    "            data['fn'].append(1)\n",
    "            data['fp'].append(np.nan)\n",
    "            data['tn'].append(np.nan)\n",
    "        # Checking ig the prediction is done\n",
    "        elif nms_predictions[0].numel()!=0:\n",
    "            # In this case we check for IOU:\n",
    "            # First we check of the number of predictions are 1 or more:\n",
    "#             my_iou_list=[]\n",
    "#             confidence= []\n",
    "            \n",
    "            if len(nms_predictions[0])>1:\n",
    "                for j in range(len(nms_predictions[0])):\n",
    "                    my_iou, conf= intersection_over_union(nms_predictions[0][j], boxes_labels, box_format= \"corners\")\n",
    "#                     print(conf)\n",
    "                    if my_iou> iou_thres:\n",
    "                        data['image_id']= image_id.tolist()\n",
    "                        data['gt_flag'].append(0)\n",
    "                        data['pd_flag'].append(0)\n",
    "                        data['confidence'].append(conf[0].tolist())\n",
    "                        data['iou'].append(my_iou[0][0].tolist())\n",
    "                        data['tp'].append(1)\n",
    "                        data['fn'].append(np.nan)\n",
    "                        data['fp'].append(np.nan)\n",
    "                        data['tn'].append(np.nan)\n",
    "                    else:\n",
    "                        data['image_id']= image_id.tolist()\n",
    "                        data['gt_flag'].append(0)\n",
    "                        data['pd_flag'].append(0)\n",
    "                        data['confidence'].append(conf[0].tolist())\n",
    "                        data['iou'].append(my_iou[0][0].tolist())\n",
    "                        data['tp'].append(np.nan)\n",
    "                        data['fn'].append(np.nan)\n",
    "                        data['fp'].append(1)\n",
    "                        data['tn'].append(np.nan)\n",
    "                        \n",
    "#                     my_iou_list.append(my_iou)\n",
    "#                     confidence.append(conf)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                my_iou, conf= intersection_over_union(nms_predictions[0], boxes_labels, box_format= \"corners\")\n",
    "                \n",
    "#                 print(conf)\n",
    "                \n",
    "                if my_iou> iou_thres:\n",
    "                    data['image_id']= image_id.tolist()\n",
    "                    data['gt_flag'].append(0)\n",
    "                    data['pd_flag'].append(0)\n",
    "                    data['confidence'].append(conf[0][0].tolist())\n",
    "                    data['iou'].append(my_iou[0][0].tolist())\n",
    "                    data['tp'].append(1)\n",
    "                    data['fn'].append(np.nan)\n",
    "                    data['fp'].append(np.nan)\n",
    "                    data['tn'].append(np.nan)\n",
    "                else:\n",
    "                    data['image_id']= image_id.tolist()\n",
    "                    data['gt_flag'].append(0)\n",
    "                    data['pd_flag'].append(0)\n",
    "                    data['confidence'].append(conf[0][0].tolist())\n",
    "                    data['iou'].append(my_iou[0][0].tolist())\n",
    "                    data['tp'].append(np.nan)\n",
    "                    data['fn'].append(np.nan)\n",
    "                    data['fp'].append(1)\n",
    "                    data['tn'].append(np.nan)\n",
    "#                 my_iou_list.append(my_iou)\n",
    "#                 confidence.append(conf)\n",
    "\n",
    "    df= pd.DataFrame(data)\n",
    "#     display(df)\n",
    "    df_list.append(df)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df= pd.concat(df_list).reset_index().drop(columns='index')\n",
    "final_df_sort= final_df.sort_values('confidence', ascending=False).reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_sort['tp_fp_fn'] = np.where((final_df_sort['tp'] ==1.0) , 'TP', \n",
    "                        np.where((final_df_sort['fp'] ==1.0) , 'FP', \n",
    "                                 np.where((final_df_sort['tn'] ==1.0) , 'TN',\n",
    "                                          np.where((final_df_sort['fn'] ==1.0) , 'FN', np.nan))))\n",
    "\n",
    "###############################################\n",
    "\n",
    "final_df_sort['tp'] = final_df_sort['tp_fp_fn'].apply(lambda x: 1 if x == 'TP' else 0)\n",
    "final_df_sort['fp'] = final_df_sort['tp_fp_fn'].apply(lambda x: 1 if x == 'FP' else 0)\n",
    "final_df_sort['fn'] = final_df_sort['tp_fp_fn'].apply(lambda x: 1 if x == 'FN' else 0)\n",
    "final_df_sort['tn'] = final_df_sort['tp_fp_fn'].apply(lambda x: 1 if x == 'TN' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75bfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "tp = np.cumsum(final_df_sort['tp_fp_fn'] == 'TP')\n",
    "fp = np.cumsum(final_df_sort['tp_fp_fn'] == 'FP')\n",
    "fn = np.sum(final_df_sort['tp_fp_fn'] == 'FN')\n",
    "\n",
    "# Calculate precision and recall at each threshold\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "final_df_sort['precision']= precision\n",
    "final_df_sort['recall']= recall\n",
    "auc_pr = auc(final_df_sort['recall'], final_df_sort['precision'])\n",
    "\n",
    "# Or can use the:\n",
    "auc= torch.trapz(torch.tensor(final_df_sort['precision'].values), torch.tensor(final_df_sort['recall'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93769452",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27221a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_pr, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce6e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(final_df_sort['recall'], final_df_sort['precision'])\n",
    "plt.title('Precision vs Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c28bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov7_custon",
   "language": "python",
   "name": "yolov7_custon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
